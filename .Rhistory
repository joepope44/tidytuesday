mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num) %>%
group_by(article_num) %>%
count() %>%
arrange(desc(n))
# extract all articles involved and create new rows per each article
clean <- violations %>%
mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num)
table(gdpr_text$chapter_title)
# extract all articles involved and create new rows per each article
clean <- violations %>%
mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num) %>%
left_join(select(gdpr_text, article_title, chapter_title), by = c("article_num" = "article"))
# extract all articles involved and create new rows per each article
clean <- violations %>%
mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num) %>%
left_join(select(gdpr_text, article, article_title, chapter_title), by = c("article_num" = "article"))
str(gdpr_text)
# extract all articles involved and create new rows per each article
clean <- violations %>%
mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num) %>%
mutate(article_num = as.integer(article_num)) %>%
left_join(select(gdpr_text, article, article_title, chapter_title), by = c("article_num" = "article"))
View(clean)
# extract all articles involved and create new rows per each article
clean <- violations %>%
mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num) %>%
mutate(article_num = as.integer(article_num)) %>%
inner_join(select(gdpr_text, article, article_title, chapter_title), by = c("article_num" = "article"))
View(clean)
clean <- violations %>%
mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num) %>%
mutate(article_num = as.integer(article_num))
# aggregate gdpr text
text <- gdpr_text %>%
group_by(article, article_title)
View(text)
# aggregate gdpr text
text <- gdpr_text %>%
group_by(article, article_title) %>%
slice(1)
# aggregate gdpr text
text <- gdpr_text %>%
group_by(article, article_title) %>%
slice(1)
# aggregate gdpr text
text <- gdpr_text %>%
group_by(chapter_title, article, article_title) %>%
slice(1)
View(gdpr_text)
# aggregate gdpr text
text <- gdpr_text %>%
group_by(chapter_title, article, article_title) %>%
slice(1) %>%
ungroup()
# extract all articles involved and create new rows per each article
clean <- violations %>%
mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num) %>%
mutate(article_num = as.integer(article_num)) %>%
inner_join(select(text, article, article_title, chapter_title), by = c("article_num" = "article"))
View(clean)
# extract all articles involved and create new rows per each article
clean <- violations %>%
mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num) %>%
mutate(article_num = as.integer(article_num)) %>%
left_join(select(text, article, article_title, chapter_title), by = c("article_num" = "article"))
source("setup.R")
tuesdata <- tidytuesdayR::tt_load(2020, week = 17)
gdpr_text <- tuesdata$gdpr_text
gdpr_violations <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-21/gdpr_violations.tsv')
table(gdpr_text$chapter_title)
gdpr_violations %>%
ggplot(aes(name, price, group = name)) +
geom_col() +
coord_flip()
violations <- gdpr_violations %>%
mutate(date = lubridate::mdy(date), name = factor(name)) %>%
filter(lubridate::year(date) > '2018')
# aggregate gdpr text
text <- gdpr_text %>%
group_by(chapter_title, article, article_title) %>%
slice(1) %>%
ungroup()
# extract all articles involved and create new rows per each article
clean <- violations %>%
mutate(article_num = str_extract_all(string = article_violated,
pattern = "(?<=Art. )(\\d+)")) %>%
unnest(article_num) %>%
mutate(article_num = as.integer(article_num)) %>%
left_join(select(text, article, article_title, chapter_title), by = c("article_num" = "article"))
View(clean)
View(gdpr_violations)
# mode
gdpr_violations %>%
group_by(controller) %>%
count(id)
# mode
gdpr_violations %>%
group_by(controller) %>%
count(id) %>%
arrange(desc(n))
# mode
gdpr_violations %>%
group_by(controller) %>%
count(id) %>%
arrange(desc(n))
gdpr_violations %>%
group_by(controller) %>%
count(id)
gdpr_violations %>%
group_by(controller) %>%
tally(id)
# mode
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n))
# mode of controllers/companies
gdpr_violations %>%
group_by(authority) %>%
tally(id) %>%
arrange(desc(n))
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
gggplot(aes(controller, n)) +
geom_col()
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
ggplot(aes(controller, n)) +
geom_col()
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(10) %>%
ggplot(aes(controller, n)) +
geom_col()
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(10)
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n))
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(controller, n)) +
geom_col()
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_relevel(controller, n), n)) +
geom_col()
# mode of controllers/companies
gdpr_violations %>%
group_by(authority) %>%
tally(id) %>%
arrange(desc(n))
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(controller, n)) +
geom_col() +
coord_flip()
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_reorder(controller, n), n)) +
geom_col() +
coord_flip()
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_reorder(controller, n), n), color = controller) +
geom_col() +
coord_flip()
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_reorder(controller, n), n, color = controller)) +
geom_col() +
coord_flip()
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_reorder(controller, n), n, fill = controller)) +
geom_col() +
coord_flip()
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_reorder(controller, n), n, fill = controller)) +
geom_col() +
coord_flip() +
theme(legend.position = "none")
# mode of controllers/companies
gdpr_violations %>%
group_by(authority) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_reorder(authority, n), n, fill = authority)) +
geom_col() +
coord_flip() +
theme(legend.position = "none")
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_reorder(controller, n), n, fill = controller)) +
geom_col() +
coord_flip() +
theme(legend.position = "none")
# mode of controllers/companies
gdpr_violations %>%
group_by(controller) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_reorder(controller, n), n, fill = controller)) +
geom_col() +
coord_flip() +
theme(legend.position = "none") +
xlab("") +
ylab("")
# mode of controllers/companies
gdpr_violations %>%
group_by(authority) %>%
tally(id) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(fct_reorder(authority, n), n, fill = authority)) +
geom_col() +
coord_flip() +
theme(legend.position = "none") +
xlab("") +
ylab("")
source('draft.R')
source('draft.R')
clean
library(tidytext)
source('~/Projects/tidytuesday/nlp.R', echo=TRUE)
word_count <- clean %>%
unnest_tokens(word, summary) %>%
anti_join(stop_words)
View(word_count)
source('~/Projects/tidytuesday/nlp.R', echo=TRUE)
word_count %>%
count(word, sort = TRUE)
summary_tokens <- clean %>%
unnest_tokens(word, summary, ) %>%
anti_join(stop_words)
summary_tokens %>%
count(word, sort = TRUE)
word_pairs <- summary_tokens %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
library(widyr)
install.packages("widyr")
library(widyr)
word_pairs <- summary_tokens %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
word_paris
word_pairs
library(igraph)
install.packages("igraph")
library(igraph)
library(ggraph)
install.packages(ggraph)
install.packages("ggraph")
library(ggraph)
desc_word_pairs <- word_pairs %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
set.seed(1234)
summ_word_pairs %>%
filter(n >= 250) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
word_pairs %>%
filter(n >= 250) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
word_pairs
word_pairs %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
word_pairs %>%
filter(n >= 2) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
word_pairs %>%
filter(n >= 10) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
word_pairs %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
summary_tokens <- clean %>%
unnest_tokens(word, summary, to_lower = TRUE, ngrams = c(1, 2)) %>%
anti_join(stop_words)
summary_tokens <- clean %>%
unnest_tokens(word, summary, to_lower = TRUE, n_grams = c(1, 2)) %>%
anti_join(stop_words)
summary_tokens <- clean %>%
unnest_tokens(word, summary, to_lower = TRUE, token = "ngrams",
n = c(1, 2)) %>%
anti_join(stop_words)
summary_tokens <- clean %>%
unnest_tokens(word, summary, to_lower = TRUE, token = "ngrams",
n_min = 1, n = 2) %>%
anti_join(stop_words)
View(summary_tokens)
summary_tokens <- clean %>%
unnest_tokens(word, summary, to_lower = TRUE, token = "ngrams",
n = 2, ) %>%
filter(is.na(as.numeric(word))) %>%
anti_join(stop_words)
summary_tokens <- clean %>%
unnest_tokens(word, summary, to_lower = TRUE) %>%
filter(is.na(as.numeric(word))) %>%
anti_join(stop_words)
word_pairs <- summary_tokens %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
word_pairs
word_pairs %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
summ_words <- summary_tokens %>%
bind_tf_idf(word, id, n)
summ_words <- summary_tokens %>%
bind_tf_idf(word, id, n)
summ_words <- summary_tokens %>%
bind_tf_idf(word, n)
df <- summary_tokens %>%
count(word, sort = TRUE)
View(df)
library(textminer)
# create a document term matrix
dtm <- CreateDtm(doc_vec = clean$summary, # character vector of documents
doc_names = clean$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(stopwords::stopwords("en"), # stopwords from tm
stopwords::stopwords(source = "smart")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = TRUE, # Turn off status bar for this demo
cpus = 4) # default is all available cpus on the system
library(textmineR)
install.packages("textmineR")
library(textmineR)
# create a document term matrix
dtm <- CreateDtm(doc_vec = clean$summary, # character vector of documents
doc_names = clean$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(stopwords::stopwords("en"), # stopwords from tm
stopwords::stopwords(source = "smart")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = TRUE, # Turn off status bar for this demo
cpus = 4) # default is all available cpus on the system
install.packages("text2vec")
library(updateR)
install_github('andreacirilloac/updateR')
install.packages('devtools') #assuming it is not already installed
install.packages("devtools")
install.packages("devtools")
library(devtools)
install_github('andreacirilloac/updateR')
updateR(admin_password = '1234qwer')
library(updateR)
updateR(admin_password = '1234qwer')
1234qwer
install.packages(as.vector(needed_packages))
sessionInfo()
source('draft.R')
library(tidytext)
summary_tokens <- clean %>%
unnest_tokens(word, summary, to_lower = TRUE) %>%
filter(is.na(as.numeric(word))) %>%
anti_join(stop_words)
df <- summary_tokens %>%
count(word, sort = TRUE)
# pairwise ----------------------------------------------------------------
library(widyr)
word_pairs <- summary_tokens %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
word_pairs
library(igraph)
library(ggraph)
set.seed(1234)
word_pairs %>%
filter(n >= 20) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
# textminer ---------------------------------------------------------------
library(textmineR)
# create a document term matrix
dtm <- CreateDtm(doc_vec = clean$summary, # character vector of documents
doc_names = clean$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(stopwords::stopwords("en"), # stopwords from tm
stopwords::stopwords(source = "smart")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = TRUE, # Turn off status bar for this demo
cpus = 4) # default is all available cpus on the system
dtm <- dtm[,colSums(dtm) > 2]
library(textmineR)
install.packages("textmineR")
install.packages("tidyverse")
source('draft.R')
library(tidyverse)
install.packages("xml2")
library(tidyverse)
